---
title: "Vingette: LMMFun"
author: "Dillon Frisco"
date: "6/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(lme4)
library(ggplot2)
library(caret)
library(tidyverse)
```


This package provides an overview of my R package designed to help with Linear Mixed Modeling for prediction. These functions will assist in the

- Plot of fixed effects with random effects
- Cross validation splits(Validation Set, K-folds, Leave-One-Out)
- Fitting linear mixed models


The primary engine for linear mixed model fitting will be the lme4 package.

This is example *sleepstudy* data set comes from the lme4 package.

Data set description:
- Data frame with 180 observations
Variables:
- Reaction : Average reaction time (ms)
- Days : Number of days of sleep deprivation
- Subject : Subject number on which the observation was made

```{r}

data <- sleepstudy

head(data)

```

Because we are doing a linear mixed model analysis lets visualize our data with respect to our potential random effects. In this case we will use Days and Subject. For visualization we use a function that makes use of the ggplot2 package.

```{r}


random_effect_visual <- function(data, 
                                 independent_var,
                                 continous_var, 
                                 identifier){
p <- ggplot(data)+
        geom_point(aes_string(x = paste(continous_var), 
                              y = paste(independent_var)))+
                facet_wrap(paste("~", identifier))
  p      
}

random_effect_visual(data = data,
independent_var = "Reaction",
continous_var = "Days",
identifier = "Subject")

```

Next we look at our function that assists in data splits for cross validation. 

Function for cross validation
-arguments
        -data
        -test/train vs. leave-one-out
        -split
-output list
        - training data
        - test data
        
```{r}
# k- fold cross validation

data = data
k = 10
identifiers = "Subject"

k_fold_lmm_split <- function(data, 
                             k = 5,
                             identifiers){
        
   unique_ids <- unique(data[[paste(identifiers)]])
      
   folds <- createFolds(unique(unique_ids), k = 10)
   
   folds
}

folds <- k_fold_lmm_split(data = data, 
                 k = 10, 
                 identifiers = "Subject")

# Validation split approach
        # defaults to a 
identifiers <-  "Subject"

validation_set_lmm_split <- function(data, identifiers, train_split = .7){
   # Default is 70 30 split
        
# Extract unique ids  
   unique_ids <- unique(data[[paste(identifiers)]])
 
# Train split
 
   train_sample <- sample(1:length(unique_ids), size = round(.7*10))
   
   train_sample
}

train_set <- validation_set_lmm_split(data, identifiers = "Subject")

```

Function for linear mixed model fitting

-arguments
        - data
        - formula
                - fixed effect
                - random effect
- output 
        - trained model object
        - summary


Now we take the data from the validation-fold cross validation splitting and use it to fit linear mixed models.

```{r validation set}


data
train_set
outcome = "Reaction"
fixed_effect = "Days"
random_slope = NULL
random_intercept = "Subject"

lmm_validation_set <- function(data, identifiers, train_set, outcome, fixed_effect, random_slope, random_intercept){

        unique_ids <- unique(data[[paste(identifiers)]])
# extract training data

        # select ids with found in the training set
 subject_vector <- data[, random_intercept] %in% unique_ids[train_set]

# extract training set
 lmm_train_data <- data[subject_vector,]

# extract test data
 lmm_test_data <- data[!subject_vector,]

# create random effect formula
 if(is.null(random_slope)){
random_effect_form <- paste("(", 1, "|",random_intercept,")")
 } else {
        random_effect_form <-  paste("(", random_slope, "|",random_intercept,")")
}

 # create fixed effect formula 
 if(length(fixed_effect) == 1){
        fixed_effect_form <- paste(fixed_effect)
        } else{
         fixed_effect_form <- paste(fixed_effect, collapse = "+")}

# create full linear mixed model formula
lmm_formula <- paste(outcome, "~", fixed_effect_form, "+", random_effect_form)


if(nrow(unique(lmm_train_data[random_intercept]))>1){
#fit linear mixed model
 lmm_train_obj <- lmer(formula(lmm_formula), data = lmm_train_data)

# use trained model and test data to output predictions
 lmm_test_data$predicted <-  predict(lmm_train_obj, 
                                     newdata = lmm_test_data[, c("Days",  "Subject")], allow.new.levels = TRUE)

# calculate mean squared errors
 test_set_mse <-  mse(lmm_test_data[,"predicted"], lmm_test_data[,outcome])
 
} else{
        stop("Only one subject in train set. Cannot fit linear mixed model on one subject.")
}

output <- list(lmm_object = lmm_train_obj,
     lmm_formula = lmm_formula,
     test_mse = test_set_mse
     )

output
}


```

```{r Test Set Validation}
lmm_validation_set(data,
                        identifiers = "Subject",
                        train_set,
                        outcome = "Reaction",
                        fixed_effect = "Days",
                        random_slope = NULL,
                        random_intercept = "Subject")
```


```{r}


lmm_k_fold_validation <- function(data, unique_ids, list_folds){}

errors <- c()

for(i in seq_along(test_sample)){

# extract training data
 lmm_train_data <- data %>% filter(Subject %in% unique_ids[test_sample[[i]]])

# extract test data
 lmm_test_data <- data %>% filter(Subject %in% unique_ids[-test_sample[[i]]])

if(length(unique(lmm_train_data$Subject))>1){
        
#fit linear mixed model
 lmm_train_obj <- lmer(Reaction ~ Days + (1|Days), data = lmm_train_data)

# use trained model and test data to output predictions
 lmm_test_data$predicted <-  predict(lmm_train_obj, 
                                     newdata = lmm_test_data[, c("Days",  "Subject")], allow.new.levels = TRUE)
# calculate mean squared errors
 current_cycle_mse <-  mse(lmm_test_data$Reaction, 
     lmm_test_data$predicted)

# append mse errors
 errors <- c(errors, 
             current_cycle_mse)
} else{
# if the fold only has 1 subject or 1 level of a grouping variable then just use a linear model to predict values for other subjects
        
        #fit linear mixed model
 lmm_train_obj <- lm(Reaction ~ Days, data = lmm_train_data)

# use trained model and test data to output predictions
 lmm_test_data$predicted <-  predict(lmm_train_obj, 
                                     newdata = lmm_test_data[, c("Days",  "Subject")], allow.new.levels = TRUE)
# calculate mean squared errors
 current_cycle_mse <-  mse(lmm_test_data$Reaction, lmm_test_data$predicted)

# append mse errors
 errors <- c(errors, 
             current_cycle_mse)
}
 
 mean(errors)
 
}

errors
```



Now we take the data from the k-fold cross validation splitting and use it to fit linear mixed models.

```{r k-fold lmm}

errors <- c()

for(i in seq_along(folds)){

# extract training data
 lmm_train_data <- data %>% filter(Subject %in% unique_ids[folds[[i]]])

# extract test data
 lmm_test_data <- data %>% filter(Subject %in% unique_ids[-folds[[i]]])

if(length(unique(lmm_train_data$Subject))>1){
        
#fit linear mixed model
 lmm_train_obj <- lmer(Reaction ~ Days + (1|Days), data = lmm_train_data)

# use trained model and test data to output predictions
 lmm_test_data$predicted <-  predict(lmm_train_obj, 
                                     newdata = lmm_test_data[, c("Days",  "Subject")], allow.new.levels = TRUE)
# calculate mean squared errors
 current_cycle_mse <-  mse(lmm_test_data$Reaction, 
     lmm_test_data$predicted)

# append mse errors
 errors <- c(errors, 
             current_cycle_mse)
} else{
# if the fold only has 1 subject or 1 level of a grouping variable then just use a linear model to predict values for other subjects
        
        #fit linear mixed model
 lmm_train_obj <- lm(Reaction ~ Days, data = lmm_train_data)

# use trained model and test data to output predictions
 lmm_test_data$predicted <-  predict(lmm_train_obj, 
                                     newdata = lmm_test_data[, c("Days",  "Subject")], allow.new.levels = TRUE)
# calculate mean squared errors
 current_cycle_mse <-  mse(lmm_test_data$Reaction, lmm_test_data$predicted)

# append mse errors
 errors <- c(errors, 
             current_cycle_mse)
}
 
 mean(errors)
 
}

errors
```

looks like the test set does a better job of prediction than k-fold

```{r}
# random effect visual outcome measure

data = lmm_test_data
prediction_independent = "predicted"
continous_var = "Days"
identifier = "Subject"

random_effect_visual <- function(data, 
                                 independent_var,
                                 continous_var,
                                 identifier,
                                 prediction_independent = NULL){
        
if(is.null(prediction_independent)){p <- ggplot(data)+
        geom_point(aes_string(x = paste(continous_var), 
                              y = paste(independent_var)))+
                facet_wrap(paste("~", identifier))} else {
        p <- ggplot(data)+
        geom_point(aes_string(x = paste(continous_var), 
                              y = paste(independent_var)))+
        geom_point(aes_string(x = paste(continous_var),
                              y = paste(prediction_independent))) +
                facet_wrap(paste("~", identifier))}
  p      
}

ggplot(data)+
        geom_point(aes_string(x = paste(continous_var), 
                              y = paste(independent_var)))+
        geom_point(aes_string(x = paste(continous_var),
                              y = paste(prediction_independent)),
                   color = "blue") +
                facet_wrap(paste("~", identifier))



```
        
        
Function for lasso model
-arguments 
        - linear-mixed model 
        
- output
        - trained lassoed model object
        
Function for autocorrelation


Final comparison of models on test set
-








## References

1. Bates D, Mächler M, Bolker B, Walker S (2015). “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software, 67(1), 1–48. doi: 10.18637/jss.v067.i01

2. Gregory Belenky, Nancy J. Wesensten, David R. Thorne, Maria L. Thomas, Helen C. Sing, Daniel P. Redmond, Michael B. Russo and Thomas J. Balkin (2003) Patterns of performance degradation and restoration during sleep restriction and subsequent recovery: a sleep dose-response study. Journal of Sleep Research 12, 1–12.

